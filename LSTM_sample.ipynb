{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dataset\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "torch.manual_seed(125)\n",
    "if torch.cuda.is_available() : \n",
    "    torch.cuda.manual_seed_all(125)\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (1.0,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../080289-main/chap07/MNIST_DATASET\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53cb74cdc0d406587e32946d10407d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../080289-main/chap07/MNIST_DATASET\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../080289-main/chap07/MNIST_DATASET\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../080289-main/chap07/MNIST_DATASET\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a836d908a2e41b992f153bca9a6729b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../080289-main/chap07/MNIST_DATASET\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../080289-main/chap07/MNIST_DATASET\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../080289-main/chap07/MNIST_DATASET\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a80c59929e4f99999ed2874a1d3d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../080289-main/chap07/MNIST_DATASET\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../080289-main/chap07/MNIST_DATASET\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../080289-main/chap07/MNIST_DATASET\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a10d2c0058747a8a0a89e97dec80277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../080289-main/chap07/MNIST_DATASET\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../080289-main/chap07/MNIST_DATASET\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "download_root = '../080289-main/chap07/MNIST_DATASET'\n",
    "\n",
    "train_dataset = MNIST(download_root, transform=mnist_transform, \n",
    "                      train=True, download=True)\n",
    "valid_dataset = MNIST(download_root, transform=mnist_transform,\n",
    "                     train=False, download=True)\n",
    "test_dataset = MNIST(download_root, transform=mnist_transform,\n",
    "                    train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "valid_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "n_iters=6000\n",
    "num_epochs=n_iters/(len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM CELL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module) :\n",
    "    def __init__(self, input_size, hidden_size, bias=True) :\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.x2h = nn.Linear(input_size, 4*hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, 4*hidden_size, bias=bias)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self) :\n",
    "        std = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for w in self.parameters() :\n",
    "            w.data.uniform_(-std, std)\n",
    "            \n",
    "    def forward(self, x, hidden) :\n",
    "        hx, cx = hidden\n",
    "        x = x.view(-1, x.size(1))\n",
    "        \n",
    "        gates = self.x2h(x) + self.h2h(hx)\n",
    "        gates = gates.squeeze()\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "        \n",
    "        ingate = F.sigmoid(ingate) # 입력 게이트에 시그모이드 적용\n",
    "        forgetgate = F.sigmoid(forgetgate) # 망각 게이트에 시그모이드 적용\n",
    "        cellgate = F.tanh(cellgate) # 셀 게이트에 탄젠트 적용\n",
    "        outgate = F.sigmoid(outgate) # 출력 게이트에 시그모이드 적용\n",
    "        \n",
    "        cy = torch.mul(cx, forgetgate) + torch.mul(ingate, cellgate)\n",
    "        hy = torch.mul(outgate, F.tanh(cy))\n",
    "        \n",
    "        return (hy, cy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM cell 네트워크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module) :\n",
    "   def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, bias=True) :\n",
    "       super(LSTMModel, self).__init__()\n",
    "       self.hidden_dim = hidden_dim\n",
    "       \n",
    "       self.layer_dim = layer_dim\n",
    "       self.lstm = LSTMCell(input_dim, hidden_dim, layer_dim)\n",
    "       self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "       \n",
    "   def forward(self, x) :\n",
    "       if torch.cuda.is_available() :\n",
    "           h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).cuda())\n",
    "       else :\n",
    "           h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "           \n",
    "       if torch.cuda.is_available() :\n",
    "           c0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).cuda())\n",
    "       else :\n",
    "           c0 = Variable(torch.zeros(self.layer_dim, x.size(0), hidden_dim))\n",
    "           \n",
    "       outs = []\n",
    "       cn =  c0[0,:,:]\n",
    "       hn = h0[0,:,:]\n",
    "       \n",
    "       for seq in range(x.size(1)) :\n",
    "           hn, cn = self.lstm(x[:, seq, :], (hn, cn))\n",
    "           outs.append(hn)\n",
    "           \n",
    "       out = outs[-1].squeeze()\n",
    "       out = self.fc(out)\n",
    "       return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM 네트워크 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=28\n",
    "hidden_dim=128\n",
    "layer_dim=1\n",
    "output_dim=18\n",
    "\n",
    "model =  LSTMModel(input_dim, hidden_dim, layer_dim, output_dim) \n",
    "if torch.cuda.is_available() :\n",
    "   model.cuda()\n",
    "   \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate =0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적화 및 손실함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1956: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 500 Loss : 2.1275174617767334 Accuracy : 26.6299991607666\n",
      "Iteration : 1000 Loss : 0.6415965557098389 Accuracy : 78.63999938964844\n",
      "Iteration : 1500 Loss : 0.4460902512073517 Accuracy : 88.19999694824219\n",
      "Iteration : 2000 Loss : 0.1142461895942688 Accuracy : 92.20999908447266\n",
      "Iteration : 2500 Loss : 0.11395461857318878 Accuracy : 95.66000366210938\n",
      "Iteration : 3000 Loss : 0.07261805236339569 Accuracy : 95.54000091552734\n",
      "Iteration : 3500 Loss : 0.11127766221761703 Accuracy : 96.19999694824219\n",
      "Iteration : 4000 Loss : 0.07671014964580536 Accuracy : 97.19000244140625\n",
      "Iteration : 4500 Loss : 0.10483045876026154 Accuracy : 96.95999908447266\n",
      "Iteration : 5000 Loss : 0.014220238663256168 Accuracy : 97.3499984741211\n",
      "Iteration : 5500 Loss : 0.019368406385183334 Accuracy : 97.36000061035156\n",
      "Iteration : 6000 Loss : 0.17768767476081848 Accuracy : 97.68000030517578\n",
      "Iteration : 6500 Loss : 0.010423594154417515 Accuracy : 97.87999725341797\n",
      "Iteration : 7000 Loss : 0.06145305931568146 Accuracy : 97.87000274658203\n",
      "Iteration : 7500 Loss : 0.09943073987960815 Accuracy : 97.93000030517578\n",
      "Iteration : 8000 Loss : 0.1466248333454132 Accuracy : 98.08000183105469\n",
      "Iteration : 8500 Loss : 0.06825259327888489 Accuracy : 98.05000305175781\n",
      "Iteration : 9000 Loss : 0.019049976021051407 Accuracy : 98.12999725341797\n"
     ]
    }
   ],
   "source": [
    "seq_dim=28\n",
    "loss_list = []\n",
    "iter = 0\n",
    "for epoch in range(num_epochs) :\n",
    "   for i, (images ,labels) in enumerate(train_loader) :\n",
    "       if torch.cuda.is_available() :\n",
    "           images = Variable(images.view(-1, seq_dim, input_dim).cuda())\n",
    "           labels = Variable(labels.cuda())\n",
    "       \n",
    "       else :\n",
    "           images = Variable(images.view(-1, seq_dim, input_dim))\n",
    "           labels = Variable(labels)\n",
    "           \n",
    "       optimizer.zero_grad()\n",
    "       outputs = model(images)\n",
    "       loss = criterion(outputs, labels)\n",
    "       \n",
    "       if torch.cuda.is_available() :\n",
    "           loss.cuda()\n",
    "           \n",
    "       loss.backward()\n",
    "       optimizer.step()\n",
    "       loss_list.append(loss.item())\n",
    "       iter += 1\n",
    "       \n",
    "       \n",
    "       if iter % 500 == 0 :\n",
    "           correct = 0\n",
    "           total = 0\n",
    "           \n",
    "           for images, labels in valid_loader :\n",
    "               \n",
    "               if torch.cuda.is_available() :\n",
    "                   images = Variable(images.view(-1, seq_dim, input_dim).cuda())\n",
    "               else :\n",
    "                   images = Variable(images.view(-1, seq_dim, input_dim))\n",
    "                   \n",
    "               outputs = model(images)\n",
    "               _, predicted = torch.max(outputs.data, 1)\n",
    "               \n",
    "               total += labels.size(0)\n",
    "               if torch.cuda.is_available() : \n",
    "                   correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                   \n",
    "               else :\n",
    "                   correct += (predicted == labels).sum()\n",
    "                   \n",
    "           accuracy = 100 * correct / total\n",
    "           print(f'Iteration : {iter} Loss : {loss.item()} Accuracy : {accuracy}')\n",
    "           "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습 및 성능 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter) :\n",
    "    corrects, total, total_loss = 0, 0, 0\n",
    "    model.eval()\n",
    "    for images, labels in val_iter :\n",
    "        if torch.cuda.is_available() :\n",
    "            images = Variable(images.view(-1, seq_dim, input_dim).cuda())\n",
    "        else :\n",
    "            images = Variable(images.view(-1, seq_dim, input_dim).to(device))\n",
    "            \n",
    "        logit = model(images).to(device)\n",
    "        loss = F.cross_entropy(logit, labels, reduction='sum')\n",
    "        _, predicted = torch.max(logit.data, 1)\n",
    "        total += labels.size(0)\n",
    "        total_loss += loss.item()\n",
    "        corrects += (predicted == labels).sum()\n",
    "        \n",
    "    avg_loss = total_loss / len(val_iter.dataset)\n",
    "    avg_accuracy = corrects / total\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 데이터셋 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss :  0.06 | Test Accuracy :  0.98\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader)\n",
    "print(f'Test Loss : {test_loss:5.2f} | Test Accuracy : {test_acc:5.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# 실제 데이터 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/User/github/Quant/data/kospi_category.csv\", encoding='cp949')\n",
    "start_date = '2004-01-01'\n",
    "end_date = '2020-03-01'\n",
    " \n",
    "data.index = pd.date_range(start_date,end_date,freq='m')\n",
    "\n",
    "data.drop('date',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>제조업</th>\n",
       "      <th>음식료품</th>\n",
       "      <th>섬유의복</th>\n",
       "      <th>종이목재</th>\n",
       "      <th>화학</th>\n",
       "      <th>의약품</th>\n",
       "      <th>비금속광물</th>\n",
       "      <th>철강금속</th>\n",
       "      <th>기계</th>\n",
       "      <th>전기전자</th>\n",
       "      <th>...</th>\n",
       "      <th>유통업</th>\n",
       "      <th>전기가스업</th>\n",
       "      <th>건설업</th>\n",
       "      <th>운수창고업</th>\n",
       "      <th>통신업</th>\n",
       "      <th>금융업</th>\n",
       "      <th>은행</th>\n",
       "      <th>증권</th>\n",
       "      <th>보험</th>\n",
       "      <th>서비스업</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-01-31</th>\n",
       "      <td>1481.36</td>\n",
       "      <td>1301.41</td>\n",
       "      <td>87.67</td>\n",
       "      <td>181.95</td>\n",
       "      <td>1030.53</td>\n",
       "      <td>1181.26</td>\n",
       "      <td>850.49</td>\n",
       "      <td>1788.47</td>\n",
       "      <td>404.42</td>\n",
       "      <td>5205.42</td>\n",
       "      <td>...</td>\n",
       "      <td>224.08</td>\n",
       "      <td>604.60</td>\n",
       "      <td>63.13</td>\n",
       "      <td>972.18</td>\n",
       "      <td>335.00</td>\n",
       "      <td>263.87</td>\n",
       "      <td>191.01</td>\n",
       "      <td>1069.30</td>\n",
       "      <td>5300.97</td>\n",
       "      <td>355.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-02-29</th>\n",
       "      <td>1534.92</td>\n",
       "      <td>1273.45</td>\n",
       "      <td>87.14</td>\n",
       "      <td>204.97</td>\n",
       "      <td>1116.18</td>\n",
       "      <td>1269.49</td>\n",
       "      <td>832.06</td>\n",
       "      <td>1963.28</td>\n",
       "      <td>439.56</td>\n",
       "      <td>5379.51</td>\n",
       "      <td>...</td>\n",
       "      <td>223.29</td>\n",
       "      <td>621.45</td>\n",
       "      <td>65.84</td>\n",
       "      <td>944.13</td>\n",
       "      <td>349.87</td>\n",
       "      <td>284.40</td>\n",
       "      <td>203.09</td>\n",
       "      <td>1121.27</td>\n",
       "      <td>5538.77</td>\n",
       "      <td>368.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-03-31</th>\n",
       "      <td>1566.62</td>\n",
       "      <td>1258.50</td>\n",
       "      <td>82.87</td>\n",
       "      <td>206.01</td>\n",
       "      <td>1110.90</td>\n",
       "      <td>1241.57</td>\n",
       "      <td>807.16</td>\n",
       "      <td>1851.05</td>\n",
       "      <td>444.92</td>\n",
       "      <td>5685.00</td>\n",
       "      <td>...</td>\n",
       "      <td>216.23</td>\n",
       "      <td>611.06</td>\n",
       "      <td>65.94</td>\n",
       "      <td>888.41</td>\n",
       "      <td>325.26</td>\n",
       "      <td>270.62</td>\n",
       "      <td>187.51</td>\n",
       "      <td>1064.50</td>\n",
       "      <td>5818.82</td>\n",
       "      <td>389.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-04-30</th>\n",
       "      <td>1523.14</td>\n",
       "      <td>1334.90</td>\n",
       "      <td>83.46</td>\n",
       "      <td>189.07</td>\n",
       "      <td>1139.42</td>\n",
       "      <td>1277.78</td>\n",
       "      <td>835.93</td>\n",
       "      <td>1674.74</td>\n",
       "      <td>400.92</td>\n",
       "      <td>5533.12</td>\n",
       "      <td>...</td>\n",
       "      <td>253.01</td>\n",
       "      <td>603.30</td>\n",
       "      <td>66.44</td>\n",
       "      <td>802.05</td>\n",
       "      <td>307.35</td>\n",
       "      <td>260.67</td>\n",
       "      <td>182.69</td>\n",
       "      <td>996.81</td>\n",
       "      <td>5895.44</td>\n",
       "      <td>417.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-05-31</th>\n",
       "      <td>1400.71</td>\n",
       "      <td>1218.45</td>\n",
       "      <td>75.89</td>\n",
       "      <td>169.21</td>\n",
       "      <td>1091.50</td>\n",
       "      <td>1146.92</td>\n",
       "      <td>736.18</td>\n",
       "      <td>1586.80</td>\n",
       "      <td>359.56</td>\n",
       "      <td>5006.79</td>\n",
       "      <td>...</td>\n",
       "      <td>265.51</td>\n",
       "      <td>585.04</td>\n",
       "      <td>59.31</td>\n",
       "      <td>774.69</td>\n",
       "      <td>298.39</td>\n",
       "      <td>237.73</td>\n",
       "      <td>171.75</td>\n",
       "      <td>844.79</td>\n",
       "      <td>5455.42</td>\n",
       "      <td>400.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>4971.85</td>\n",
       "      <td>3410.15</td>\n",
       "      <td>282.12</td>\n",
       "      <td>338.47</td>\n",
       "      <td>4559.73</td>\n",
       "      <td>10792.64</td>\n",
       "      <td>1507.52</td>\n",
       "      <td>3508.32</td>\n",
       "      <td>698.16</td>\n",
       "      <td>18208.01</td>\n",
       "      <td>...</td>\n",
       "      <td>350.33</td>\n",
       "      <td>825.45</td>\n",
       "      <td>93.38</td>\n",
       "      <td>1280.08</td>\n",
       "      <td>339.66</td>\n",
       "      <td>390.70</td>\n",
       "      <td>246.57</td>\n",
       "      <td>1678.24</td>\n",
       "      <td>12886.06</td>\n",
       "      <td>1166.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>4930.53</td>\n",
       "      <td>3431.14</td>\n",
       "      <td>287.74</td>\n",
       "      <td>330.44</td>\n",
       "      <td>4479.44</td>\n",
       "      <td>10229.68</td>\n",
       "      <td>1468.46</td>\n",
       "      <td>3605.86</td>\n",
       "      <td>673.26</td>\n",
       "      <td>18186.96</td>\n",
       "      <td>...</td>\n",
       "      <td>355.86</td>\n",
       "      <td>878.30</td>\n",
       "      <td>91.72</td>\n",
       "      <td>1298.96</td>\n",
       "      <td>348.64</td>\n",
       "      <td>401.50</td>\n",
       "      <td>245.59</td>\n",
       "      <td>1709.26</td>\n",
       "      <td>13464.64</td>\n",
       "      <td>1176.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>5273.05</td>\n",
       "      <td>3480.12</td>\n",
       "      <td>289.14</td>\n",
       "      <td>330.11</td>\n",
       "      <td>4591.69</td>\n",
       "      <td>11031.00</td>\n",
       "      <td>1489.51</td>\n",
       "      <td>3693.15</td>\n",
       "      <td>701.59</td>\n",
       "      <td>20207.86</td>\n",
       "      <td>...</td>\n",
       "      <td>369.33</td>\n",
       "      <td>876.02</td>\n",
       "      <td>94.07</td>\n",
       "      <td>1330.05</td>\n",
       "      <td>345.69</td>\n",
       "      <td>409.49</td>\n",
       "      <td>246.62</td>\n",
       "      <td>1739.59</td>\n",
       "      <td>13698.26</td>\n",
       "      <td>1219.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-31</th>\n",
       "      <td>5160.22</td>\n",
       "      <td>3295.05</td>\n",
       "      <td>259.04</td>\n",
       "      <td>352.95</td>\n",
       "      <td>4267.39</td>\n",
       "      <td>10902.30</td>\n",
       "      <td>1408.72</td>\n",
       "      <td>3395.63</td>\n",
       "      <td>649.76</td>\n",
       "      <td>20403.97</td>\n",
       "      <td>...</td>\n",
       "      <td>350.09</td>\n",
       "      <td>794.12</td>\n",
       "      <td>86.48</td>\n",
       "      <td>1250.76</td>\n",
       "      <td>329.30</td>\n",
       "      <td>372.14</td>\n",
       "      <td>224.73</td>\n",
       "      <td>1596.21</td>\n",
       "      <td>12249.88</td>\n",
       "      <td>1202.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-29</th>\n",
       "      <td>4893.83</td>\n",
       "      <td>3178.68</td>\n",
       "      <td>232.81</td>\n",
       "      <td>320.97</td>\n",
       "      <td>4037.85</td>\n",
       "      <td>10565.78</td>\n",
       "      <td>1340.51</td>\n",
       "      <td>3147.69</td>\n",
       "      <td>608.82</td>\n",
       "      <td>19573.08</td>\n",
       "      <td>...</td>\n",
       "      <td>324.40</td>\n",
       "      <td>678.05</td>\n",
       "      <td>78.51</td>\n",
       "      <td>1226.31</td>\n",
       "      <td>310.52</td>\n",
       "      <td>334.07</td>\n",
       "      <td>198.91</td>\n",
       "      <td>1467.80</td>\n",
       "      <td>11085.54</td>\n",
       "      <td>1129.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                제조업     음식료품    섬유의복    종이목재       화학       의약품    비금속광물  \\\n",
       "2004-01-31  1481.36  1301.41   87.67  181.95  1030.53   1181.26   850.49   \n",
       "2004-02-29  1534.92  1273.45   87.14  204.97  1116.18   1269.49   832.06   \n",
       "2004-03-31  1566.62  1258.50   82.87  206.01  1110.90   1241.57   807.16   \n",
       "2004-04-30  1523.14  1334.90   83.46  189.07  1139.42   1277.78   835.93   \n",
       "2004-05-31  1400.71  1218.45   75.89  169.21  1091.50   1146.92   736.18   \n",
       "...             ...      ...     ...     ...      ...       ...      ...   \n",
       "2019-10-31  4971.85  3410.15  282.12  338.47  4559.73  10792.64  1507.52   \n",
       "2019-11-30  4930.53  3431.14  287.74  330.44  4479.44  10229.68  1468.46   \n",
       "2019-12-31  5273.05  3480.12  289.14  330.11  4591.69  11031.00  1489.51   \n",
       "2020-01-31  5160.22  3295.05  259.04  352.95  4267.39  10902.30  1408.72   \n",
       "2020-02-29  4893.83  3178.68  232.81  320.97  4037.85  10565.78  1340.51   \n",
       "\n",
       "               철강금속      기계      전기전자  ...     유통업   전기가스업    건설업    운수창고업  \\\n",
       "2004-01-31  1788.47  404.42   5205.42  ...  224.08  604.60  63.13   972.18   \n",
       "2004-02-29  1963.28  439.56   5379.51  ...  223.29  621.45  65.84   944.13   \n",
       "2004-03-31  1851.05  444.92   5685.00  ...  216.23  611.06  65.94   888.41   \n",
       "2004-04-30  1674.74  400.92   5533.12  ...  253.01  603.30  66.44   802.05   \n",
       "2004-05-31  1586.80  359.56   5006.79  ...  265.51  585.04  59.31   774.69   \n",
       "...             ...     ...       ...  ...     ...     ...    ...      ...   \n",
       "2019-10-31  3508.32  698.16  18208.01  ...  350.33  825.45  93.38  1280.08   \n",
       "2019-11-30  3605.86  673.26  18186.96  ...  355.86  878.30  91.72  1298.96   \n",
       "2019-12-31  3693.15  701.59  20207.86  ...  369.33  876.02  94.07  1330.05   \n",
       "2020-01-31  3395.63  649.76  20403.97  ...  350.09  794.12  86.48  1250.76   \n",
       "2020-02-29  3147.69  608.82  19573.08  ...  324.40  678.05  78.51  1226.31   \n",
       "\n",
       "               통신업     금융업      은행       증권        보험     서비스업  \n",
       "2004-01-31  335.00  263.87  191.01  1069.30   5300.97   355.96  \n",
       "2004-02-29  349.87  284.40  203.09  1121.27   5538.77   368.91  \n",
       "2004-03-31  325.26  270.62  187.51  1064.50   5818.82   389.92  \n",
       "2004-04-30  307.35  260.67  182.69   996.81   5895.44   417.70  \n",
       "2004-05-31  298.39  237.73  171.75   844.79   5455.42   400.59  \n",
       "...            ...     ...     ...      ...       ...      ...  \n",
       "2019-10-31  339.66  390.70  246.57  1678.24  12886.06  1166.28  \n",
       "2019-11-30  348.64  401.50  245.59  1709.26  13464.64  1176.93  \n",
       "2019-12-31  345.69  409.49  246.62  1739.59  13698.26  1219.07  \n",
       "2020-01-31  329.30  372.14  224.73  1596.21  12249.88  1202.74  \n",
       "2020-02-29  310.52  334.07  198.91  1467.80  11085.54  1129.52  \n",
       "\n",
       "[194 rows x 22 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>제조업</th>\n",
       "      <th>음식료품</th>\n",
       "      <th>섬유의복</th>\n",
       "      <th>종이목재</th>\n",
       "      <th>화학</th>\n",
       "      <th>의약품</th>\n",
       "      <th>비금속광물</th>\n",
       "      <th>철강금속</th>\n",
       "      <th>기계</th>\n",
       "      <th>전기전자</th>\n",
       "      <th>...</th>\n",
       "      <th>유통업</th>\n",
       "      <th>전기가스업</th>\n",
       "      <th>건설업</th>\n",
       "      <th>운수창고업</th>\n",
       "      <th>통신업</th>\n",
       "      <th>금융업</th>\n",
       "      <th>은행</th>\n",
       "      <th>증권</th>\n",
       "      <th>보험</th>\n",
       "      <th>서비스업</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>4670.68</td>\n",
       "      <td>3645.13</td>\n",
       "      <td>292.80</td>\n",
       "      <td>381.55</td>\n",
       "      <td>4669.26</td>\n",
       "      <td>9951.76</td>\n",
       "      <td>1651.96</td>\n",
       "      <td>3969.08</td>\n",
       "      <td>737.95</td>\n",
       "      <td>15669.14</td>\n",
       "      <td>...</td>\n",
       "      <td>377.70</td>\n",
       "      <td>851.31</td>\n",
       "      <td>108.39</td>\n",
       "      <td>1447.12</td>\n",
       "      <td>358.04</td>\n",
       "      <td>427.92</td>\n",
       "      <td>289.39</td>\n",
       "      <td>1808.10</td>\n",
       "      <td>15579.44</td>\n",
       "      <td>1091.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>4945.30</td>\n",
       "      <td>3740.90</td>\n",
       "      <td>298.43</td>\n",
       "      <td>391.83</td>\n",
       "      <td>4733.17</td>\n",
       "      <td>10349.47</td>\n",
       "      <td>1699.23</td>\n",
       "      <td>4145.27</td>\n",
       "      <td>765.30</td>\n",
       "      <td>17071.05</td>\n",
       "      <td>...</td>\n",
       "      <td>388.00</td>\n",
       "      <td>844.46</td>\n",
       "      <td>113.78</td>\n",
       "      <td>1363.13</td>\n",
       "      <td>368.47</td>\n",
       "      <td>438.51</td>\n",
       "      <td>293.51</td>\n",
       "      <td>1969.14</td>\n",
       "      <td>15468.15</td>\n",
       "      <td>1112.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>4710.39</td>\n",
       "      <td>3450.11</td>\n",
       "      <td>285.99</td>\n",
       "      <td>343.65</td>\n",
       "      <td>4525.94</td>\n",
       "      <td>8937.99</td>\n",
       "      <td>1506.53</td>\n",
       "      <td>3815.12</td>\n",
       "      <td>716.77</td>\n",
       "      <td>16689.78</td>\n",
       "      <td>...</td>\n",
       "      <td>353.53</td>\n",
       "      <td>901.27</td>\n",
       "      <td>98.33</td>\n",
       "      <td>1283.44</td>\n",
       "      <td>350.47</td>\n",
       "      <td>407.35</td>\n",
       "      <td>274.70</td>\n",
       "      <td>1803.42</td>\n",
       "      <td>14388.27</td>\n",
       "      <td>1106.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-31</th>\n",
       "      <td>4606.33</td>\n",
       "      <td>3251.38</td>\n",
       "      <td>266.49</td>\n",
       "      <td>364.67</td>\n",
       "      <td>4369.96</td>\n",
       "      <td>8538.84</td>\n",
       "      <td>1543.00</td>\n",
       "      <td>3638.72</td>\n",
       "      <td>709.21</td>\n",
       "      <td>16277.85</td>\n",
       "      <td>...</td>\n",
       "      <td>341.96</td>\n",
       "      <td>827.50</td>\n",
       "      <td>94.16</td>\n",
       "      <td>1290.76</td>\n",
       "      <td>342.16</td>\n",
       "      <td>379.80</td>\n",
       "      <td>259.04</td>\n",
       "      <td>1721.07</td>\n",
       "      <td>12879.06</td>\n",
       "      <td>1118.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>4896.77</td>\n",
       "      <td>3453.54</td>\n",
       "      <td>285.42</td>\n",
       "      <td>348.74</td>\n",
       "      <td>4461.52</td>\n",
       "      <td>8978.69</td>\n",
       "      <td>1522.35</td>\n",
       "      <td>3778.30</td>\n",
       "      <td>744.99</td>\n",
       "      <td>17780.19</td>\n",
       "      <td>...</td>\n",
       "      <td>349.36</td>\n",
       "      <td>837.46</td>\n",
       "      <td>100.16</td>\n",
       "      <td>1265.60</td>\n",
       "      <td>347.23</td>\n",
       "      <td>391.95</td>\n",
       "      <td>275.50</td>\n",
       "      <td>1767.69</td>\n",
       "      <td>13150.57</td>\n",
       "      <td>1138.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>4971.85</td>\n",
       "      <td>3410.15</td>\n",
       "      <td>282.12</td>\n",
       "      <td>338.47</td>\n",
       "      <td>4559.73</td>\n",
       "      <td>10792.64</td>\n",
       "      <td>1507.52</td>\n",
       "      <td>3508.32</td>\n",
       "      <td>698.16</td>\n",
       "      <td>18208.01</td>\n",
       "      <td>...</td>\n",
       "      <td>350.33</td>\n",
       "      <td>825.45</td>\n",
       "      <td>93.38</td>\n",
       "      <td>1280.08</td>\n",
       "      <td>339.66</td>\n",
       "      <td>390.70</td>\n",
       "      <td>246.57</td>\n",
       "      <td>1678.24</td>\n",
       "      <td>12886.06</td>\n",
       "      <td>1166.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>4930.53</td>\n",
       "      <td>3431.14</td>\n",
       "      <td>287.74</td>\n",
       "      <td>330.44</td>\n",
       "      <td>4479.44</td>\n",
       "      <td>10229.68</td>\n",
       "      <td>1468.46</td>\n",
       "      <td>3605.86</td>\n",
       "      <td>673.26</td>\n",
       "      <td>18186.96</td>\n",
       "      <td>...</td>\n",
       "      <td>355.86</td>\n",
       "      <td>878.30</td>\n",
       "      <td>91.72</td>\n",
       "      <td>1298.96</td>\n",
       "      <td>348.64</td>\n",
       "      <td>401.50</td>\n",
       "      <td>245.59</td>\n",
       "      <td>1709.26</td>\n",
       "      <td>13464.64</td>\n",
       "      <td>1176.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>5273.05</td>\n",
       "      <td>3480.12</td>\n",
       "      <td>289.14</td>\n",
       "      <td>330.11</td>\n",
       "      <td>4591.69</td>\n",
       "      <td>11031.00</td>\n",
       "      <td>1489.51</td>\n",
       "      <td>3693.15</td>\n",
       "      <td>701.59</td>\n",
       "      <td>20207.86</td>\n",
       "      <td>...</td>\n",
       "      <td>369.33</td>\n",
       "      <td>876.02</td>\n",
       "      <td>94.07</td>\n",
       "      <td>1330.05</td>\n",
       "      <td>345.69</td>\n",
       "      <td>409.49</td>\n",
       "      <td>246.62</td>\n",
       "      <td>1739.59</td>\n",
       "      <td>13698.26</td>\n",
       "      <td>1219.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-31</th>\n",
       "      <td>5160.22</td>\n",
       "      <td>3295.05</td>\n",
       "      <td>259.04</td>\n",
       "      <td>352.95</td>\n",
       "      <td>4267.39</td>\n",
       "      <td>10902.30</td>\n",
       "      <td>1408.72</td>\n",
       "      <td>3395.63</td>\n",
       "      <td>649.76</td>\n",
       "      <td>20403.97</td>\n",
       "      <td>...</td>\n",
       "      <td>350.09</td>\n",
       "      <td>794.12</td>\n",
       "      <td>86.48</td>\n",
       "      <td>1250.76</td>\n",
       "      <td>329.30</td>\n",
       "      <td>372.14</td>\n",
       "      <td>224.73</td>\n",
       "      <td>1596.21</td>\n",
       "      <td>12249.88</td>\n",
       "      <td>1202.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-29</th>\n",
       "      <td>4893.83</td>\n",
       "      <td>3178.68</td>\n",
       "      <td>232.81</td>\n",
       "      <td>320.97</td>\n",
       "      <td>4037.85</td>\n",
       "      <td>10565.78</td>\n",
       "      <td>1340.51</td>\n",
       "      <td>3147.69</td>\n",
       "      <td>608.82</td>\n",
       "      <td>19573.08</td>\n",
       "      <td>...</td>\n",
       "      <td>324.40</td>\n",
       "      <td>678.05</td>\n",
       "      <td>78.51</td>\n",
       "      <td>1226.31</td>\n",
       "      <td>310.52</td>\n",
       "      <td>334.07</td>\n",
       "      <td>198.91</td>\n",
       "      <td>1467.80</td>\n",
       "      <td>11085.54</td>\n",
       "      <td>1129.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                제조업     음식료품    섬유의복    종이목재       화학       의약품    비금속광물  \\\n",
       "2019-05-31  4670.68  3645.13  292.80  381.55  4669.26   9951.76  1651.96   \n",
       "2019-06-30  4945.30  3740.90  298.43  391.83  4733.17  10349.47  1699.23   \n",
       "2019-07-31  4710.39  3450.11  285.99  343.65  4525.94   8937.99  1506.53   \n",
       "2019-08-31  4606.33  3251.38  266.49  364.67  4369.96   8538.84  1543.00   \n",
       "2019-09-30  4896.77  3453.54  285.42  348.74  4461.52   8978.69  1522.35   \n",
       "2019-10-31  4971.85  3410.15  282.12  338.47  4559.73  10792.64  1507.52   \n",
       "2019-11-30  4930.53  3431.14  287.74  330.44  4479.44  10229.68  1468.46   \n",
       "2019-12-31  5273.05  3480.12  289.14  330.11  4591.69  11031.00  1489.51   \n",
       "2020-01-31  5160.22  3295.05  259.04  352.95  4267.39  10902.30  1408.72   \n",
       "2020-02-29  4893.83  3178.68  232.81  320.97  4037.85  10565.78  1340.51   \n",
       "\n",
       "               철강금속      기계      전기전자  ...     유통업   전기가스업     건설업    운수창고업  \\\n",
       "2019-05-31  3969.08  737.95  15669.14  ...  377.70  851.31  108.39  1447.12   \n",
       "2019-06-30  4145.27  765.30  17071.05  ...  388.00  844.46  113.78  1363.13   \n",
       "2019-07-31  3815.12  716.77  16689.78  ...  353.53  901.27   98.33  1283.44   \n",
       "2019-08-31  3638.72  709.21  16277.85  ...  341.96  827.50   94.16  1290.76   \n",
       "2019-09-30  3778.30  744.99  17780.19  ...  349.36  837.46  100.16  1265.60   \n",
       "2019-10-31  3508.32  698.16  18208.01  ...  350.33  825.45   93.38  1280.08   \n",
       "2019-11-30  3605.86  673.26  18186.96  ...  355.86  878.30   91.72  1298.96   \n",
       "2019-12-31  3693.15  701.59  20207.86  ...  369.33  876.02   94.07  1330.05   \n",
       "2020-01-31  3395.63  649.76  20403.97  ...  350.09  794.12   86.48  1250.76   \n",
       "2020-02-29  3147.69  608.82  19573.08  ...  324.40  678.05   78.51  1226.31   \n",
       "\n",
       "               통신업     금융업      은행       증권        보험     서비스업  \n",
       "2019-05-31  358.04  427.92  289.39  1808.10  15579.44  1091.96  \n",
       "2019-06-30  368.47  438.51  293.51  1969.14  15468.15  1112.03  \n",
       "2019-07-31  350.47  407.35  274.70  1803.42  14388.27  1106.70  \n",
       "2019-08-31  342.16  379.80  259.04  1721.07  12879.06  1118.23  \n",
       "2019-09-30  347.23  391.95  275.50  1767.69  13150.57  1138.31  \n",
       "2019-10-31  339.66  390.70  246.57  1678.24  12886.06  1166.28  \n",
       "2019-11-30  348.64  401.50  245.59  1709.26  13464.64  1176.93  \n",
       "2019-12-31  345.69  409.49  246.62  1739.59  13698.26  1219.07  \n",
       "2020-01-31  329.30  372.14  224.73  1596.21  12249.88  1202.74  \n",
       "2020-02-29  310.52  334.07  198.91  1467.80  11085.54  1129.52  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[-10:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                제조업     음식료품    섬유의복    종이목재       화학       의약품    비금속광물  \\\n",
      "2004-01-31  1481.36  1301.41   87.67  181.95  1030.53   1181.26   850.49   \n",
      "2004-02-29  1534.92  1273.45   87.14  204.97  1116.18   1269.49   832.06   \n",
      "2004-03-31  1566.62  1258.50   82.87  206.01  1110.90   1241.57   807.16   \n",
      "2004-04-30  1523.14  1334.90   83.46  189.07  1139.42   1277.78   835.93   \n",
      "2004-05-31  1400.71  1218.45   75.89  169.21  1091.50   1146.92   736.18   \n",
      "...             ...      ...     ...     ...      ...       ...      ...   \n",
      "2017-08-31  5315.28  4428.05  296.83  327.21  5842.06   9841.25  1299.77   \n",
      "2017-09-30  5515.26  4240.81  281.13  310.72  5698.48  10693.90  1185.46   \n",
      "2017-10-31  5837.07  4431.02  287.26  308.18  5994.89  11528.94  1294.92   \n",
      "2017-11-30  5687.52  4441.53  306.32  309.13  6065.43  11656.16  1443.25   \n",
      "2017-12-31  5648.76  4364.77  294.92  298.95  6071.54  12370.06  1352.51   \n",
      "\n",
      "               철강금속      기계      전기전자  ...     유통업    전기가스업     건설업    운수창고업  \\\n",
      "2004-01-31  1788.47  404.42   5205.42  ...  224.08   604.60   63.13   972.18   \n",
      "2004-02-29  1963.28  439.56   5379.51  ...  223.29   621.45   65.84   944.13   \n",
      "2004-03-31  1851.05  444.92   5685.00  ...  216.23   611.06   65.94   888.41   \n",
      "2004-04-30  1674.74  400.92   5533.12  ...  253.01   603.30   66.44   802.05   \n",
      "2004-05-31  1586.80  359.56   5006.79  ...  265.51   585.04   59.31   774.69   \n",
      "...             ...     ...       ...  ...     ...      ...     ...      ...   \n",
      "2017-08-31  5571.90  888.00  17188.85  ...  449.95  1288.94  110.54  1645.11   \n",
      "2017-09-30  5130.18  892.99  18876.18  ...  434.44  1181.62  105.23  1537.02   \n",
      "2017-10-31  5290.53  916.17  19963.43  ...  466.12  1181.84  107.41  1522.98   \n",
      "2017-11-30  5344.40  911.32  18715.98  ...  468.29  1157.11  102.08  1478.36   \n",
      "2017-12-31  5293.84  920.54  18795.89  ...  455.45  1164.06  103.47  1406.03   \n",
      "\n",
      "               통신업     금융업      은행       증권        보험     서비스업  \n",
      "2004-01-31  335.00  263.87  191.01  1069.30   5300.97   355.96  \n",
      "2004-02-29  349.87  284.40  203.09  1121.27   5538.77   368.91  \n",
      "2004-03-31  325.26  270.62  187.51  1064.50   5818.82   389.92  \n",
      "2004-04-30  307.35  260.67  182.69   996.81   5895.44   417.70  \n",
      "2004-05-31  298.39  237.73  171.75   844.79   5455.42   400.59  \n",
      "...            ...     ...     ...      ...       ...      ...  \n",
      "2017-08-31  367.35  533.58  343.17  2047.26  20997.02  1157.47  \n",
      "2017-09-30  361.03  523.02  329.59  1942.63  20500.33  1179.22  \n",
      "2017-10-31  367.05  542.56  322.29  1993.64  22212.26  1288.17  \n",
      "2017-11-30  374.70  542.30  325.21  2046.52  21433.31  1248.22  \n",
      "2017-12-31  377.91  540.29  325.34  1986.01  21138.28  1293.04  \n",
      "\n",
      "[168 rows x 22 columns]\n",
      "                제조업     음식료품    섬유의복    종이목재       화학       의약품    비금속광물  \\\n",
      "2018-01-31  5793.32  4519.07  307.05  337.45  6385.57  14179.41  1524.83   \n",
      "2018-02-28  5565.65  4208.97  279.95  344.71  6074.11  14839.06  1369.28   \n",
      "2018-03-31  5634.00  4273.76  285.77  374.29  6097.05  14672.94  1376.07   \n",
      "2018-04-30  5836.45  4451.48  298.91  399.69  6187.08  13863.89  1695.15   \n",
      "2018-05-31  5676.43  4436.98  307.39  452.77  5882.77  13284.62  2030.56   \n",
      "2018-06-30  5389.21  4628.44  312.88  406.30  5792.04  13320.24  1686.87   \n",
      "2018-07-31  5323.39  4332.49  298.93  385.94  5675.43  12281.44  1644.04   \n",
      "2018-08-31  5449.32  4063.70  327.50  444.00  5625.06  13665.52  1796.92   \n",
      "2018-09-30  5393.59  4077.32  320.06  437.57  5600.71  14878.96  1896.78   \n",
      "2018-10-31  4629.03  3665.33  257.27  351.07  4646.56  10867.84  1495.08   \n",
      "2018-11-30  4716.47  3961.63  259.91  351.57  4887.66  11381.22  1674.10   \n",
      "2018-12-31  4579.62  3960.01  253.51  357.66  4874.93  11626.69  1661.05   \n",
      "2019-01-31  5087.28  4047.11  267.78  384.20  5142.43  11773.39  1817.81   \n",
      "2019-02-28  5037.85  4014.01  286.28  390.02  5275.01  11473.64  1747.23   \n",
      "2019-03-31  4902.15  4050.36  312.50  386.45  5110.77  10302.94  1730.43   \n",
      "2019-04-30  5097.73  3991.86  318.99  397.01  5133.95  11127.72  1691.35   \n",
      "2019-05-31  4670.68  3645.13  292.80  381.55  4669.26   9951.76  1651.96   \n",
      "2019-06-30  4945.30  3740.90  298.43  391.83  4733.17  10349.47  1699.23   \n",
      "2019-07-31  4710.39  3450.11  285.99  343.65  4525.94   8937.99  1506.53   \n",
      "2019-08-31  4606.33  3251.38  266.49  364.67  4369.96   8538.84  1543.00   \n",
      "2019-09-30  4896.77  3453.54  285.42  348.74  4461.52   8978.69  1522.35   \n",
      "2019-10-31  4971.85  3410.15  282.12  338.47  4559.73  10792.64  1507.52   \n",
      "2019-11-30  4930.53  3431.14  287.74  330.44  4479.44  10229.68  1468.46   \n",
      "2019-12-31  5273.05  3480.12  289.14  330.11  4591.69  11031.00  1489.51   \n",
      "2020-01-31  5160.22  3295.05  259.04  352.95  4267.39  10902.30  1408.72   \n",
      "2020-02-29  4893.83  3178.68  232.81  320.97  4037.85  10565.78  1340.51   \n",
      "\n",
      "               철강금속      기계      전기전자  ...     유통업    전기가스업     건설업    운수창고업  \\\n",
      "2018-01-31  5783.55  999.13  18489.76  ...  498.56  1135.90  113.87  1517.66   \n",
      "2018-02-28  5489.57  918.11  17588.77  ...  456.76  1062.04  101.87  1461.37   \n",
      "2018-03-31  5067.77  930.80  18430.64  ...  473.60  1061.96  107.74  1509.77   \n",
      "2018-04-30  5542.42  964.35  19472.10  ...  489.91  1192.37  136.84  1607.39   \n",
      "2018-05-31  5227.38  978.42  19184.98  ...  471.58  1105.54  145.69  1463.05   \n",
      "2018-06-30  4875.56  871.59  17805.32  ...  459.65  1090.73  120.47  1341.21   \n",
      "2018-07-31  4898.49  855.53  17783.86  ...  431.65  1099.88  120.04  1378.95   \n",
      "2018-08-31  4877.01  876.74  18317.40  ...  429.76  1011.30  121.21  1325.65   \n",
      "2018-09-30  4713.97  925.36  17394.10  ...  452.20  1010.99  125.98  1365.18   \n",
      "2018-10-31  3972.97  736.80  15784.05  ...  378.12   919.84   97.27  1234.62   \n",
      "2018-11-30  4028.74  771.38  15678.44  ...  394.82   981.80  109.17  1333.94   \n",
      "2018-12-31  4043.12  765.14  14447.02  ...  393.50  1050.60  112.60  1341.29   \n",
      "2019-01-31  4450.89  848.14  16947.32  ...  411.92  1107.88  121.54  1402.56   \n",
      "2019-02-28  4376.72  796.47  16675.95  ...  415.66  1102.52  113.32  1435.11   \n",
      "2019-03-31  4213.82  753.79  16593.43  ...  420.20   965.78  111.53  1312.44   \n",
      "2019-04-30  4246.97  791.51  17173.67  ...  424.29   929.36  111.29  1487.28   \n",
      "2019-05-31  3969.08  737.95  15669.14  ...  377.70   851.31  108.39  1447.12   \n",
      "2019-06-30  4145.27  765.30  17071.05  ...  388.00   844.46  113.78  1363.13   \n",
      "2019-07-31  3815.12  716.77  16689.78  ...  353.53   901.27   98.33  1283.44   \n",
      "2019-08-31  3638.72  709.21  16277.85  ...  341.96   827.50   94.16  1290.76   \n",
      "2019-09-30  3778.30  744.99  17780.19  ...  349.36   837.46  100.16  1265.60   \n",
      "2019-10-31  3508.32  698.16  18208.01  ...  350.33   825.45   93.38  1280.08   \n",
      "2019-11-30  3605.86  673.26  18186.96  ...  355.86   878.30   91.72  1298.96   \n",
      "2019-12-31  3693.15  701.59  20207.86  ...  369.33   876.02   94.07  1330.05   \n",
      "2020-01-31  3395.63  649.76  20403.97  ...  350.09   794.12   86.48  1250.76   \n",
      "2020-02-29  3147.69  608.82  19573.08  ...  324.40   678.05   78.51  1226.31   \n",
      "\n",
      "               통신업     금융업      은행       증권        보험     서비스업  \n",
      "2018-01-31  376.42  577.19  339.49  2402.75  22296.29  1362.90  \n",
      "2018-02-28  342.15  540.16  341.29  2122.54  21048.96  1228.52  \n",
      "2018-03-31  340.61  525.55  303.33  2075.89  19773.83  1267.30  \n",
      "2018-04-30  338.14  529.81  331.24  2162.58  19671.63  1212.98  \n",
      "2018-05-31  327.29  495.59  311.93  2126.55  18032.94  1159.61  \n",
      "2018-06-30  347.48  481.59  320.42  1933.03  17805.02  1177.06  \n",
      "2018-07-31  370.21  475.12  329.31  1810.88  17914.73  1147.92  \n",
      "2018-08-31  385.19  468.03  316.50  1849.16  17178.26  1175.72  \n",
      "2018-09-30  416.70  491.16  325.61  1915.34  18265.86  1176.32  \n",
      "2018-10-31  387.57  438.85  309.62  1587.19  17269.98  1017.88  \n",
      "2018-11-30  418.78  448.83  308.11  1767.51  16711.13  1127.99  \n",
      "2018-12-31  397.99  434.35  300.61  1675.55  16303.29  1080.00  \n",
      "2019-01-31  373.77  451.09  293.77  1833.75  16780.72  1135.15  \n",
      "2019-02-28  375.12  453.81  291.58  1801.49  17463.20  1146.03  \n",
      "2019-03-31  365.61  442.16  293.58  1789.45  17090.50  1148.32  \n",
      "2019-04-30  356.33  451.03  295.79  1877.80  17160.48  1159.85  \n",
      "2019-05-31  358.04  427.92  289.39  1808.10  15579.44  1091.96  \n",
      "2019-06-30  368.47  438.51  293.51  1969.14  15468.15  1112.03  \n",
      "2019-07-31  350.47  407.35  274.70  1803.42  14388.27  1106.70  \n",
      "2019-08-31  342.16  379.80  259.04  1721.07  12879.06  1118.23  \n",
      "2019-09-30  347.23  391.95  275.50  1767.69  13150.57  1138.31  \n",
      "2019-10-31  339.66  390.70  246.57  1678.24  12886.06  1166.28  \n",
      "2019-11-30  348.64  401.50  245.59  1709.26  13464.64  1176.93  \n",
      "2019-12-31  345.69  409.49  246.62  1739.59  13698.26  1219.07  \n",
      "2020-01-31  329.30  372.14  224.73  1596.21  12249.88  1202.74  \n",
      "2020-02-29  310.52  334.07  198.91  1467.80  11085.54  1129.52  \n",
      "\n",
      "[26 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:-26, :]\n",
    "y = data.iloc[-26:, :]\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = MinMaxScaler()\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_ss = ss.fit_transform(X)\n",
    "y_ms = ms.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape : (84, 22) (13, 22)\n",
      "Testing Shape : (84, 22) (13, 22)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = X_ss[:84, :]\n",
    "X_test = X_ss[84:, :]\n",
    "\n",
    "y_train = y_ms[:13, :]\n",
    "y_test = y_ms[13:, :]\n",
    "\n",
    "print('Training Shape :', X_train.shape, y_train.shape)\n",
    "print('Testing Shape :', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape : torch.Size([84, 1, 22]) torch.Size([13, 22])\n",
      "Testing Shape : torch.Size([84, 1, 22]) torch.Size([13, 22])\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 형태, 크기 조정\n",
    "X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "\n",
    "y_train_tensors = Variable(torch.Tensor(y_train))\n",
    "y_test_tensors = Variable(torch.Tensor(y_test))\n",
    "\n",
    "X_train_tensors_f = torch.reshape(X_train_tensors, (X_train_tensors.shape[0],\n",
    "                                                  1, X_train_tensors.shape[1]))\n",
    "X_test_tensors_f = torch.reshape(X_test_tensors, (X_test_tensors.shape[0],\n",
    "                                                 1, X_test_tensors.shape[1]))\n",
    "\n",
    "print('Training Shape :', X_train_tensors_f.shape, y_train_tensors.shape)\n",
    "print('Testing Shape :', X_test_tensors_f.shape, y_test_tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module) :\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length) :\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                           num_layers=num_layers, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_size, 128)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0))\n",
    "        hn = hn.view(-1, self.hidden_size)\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수값 설정\n",
    "num_epochs = 1000\n",
    "lr = 0.0001\n",
    "\n",
    "input_size=22\n",
    "hidden_size=2\n",
    "num_layers=1\n",
    "\n",
    "num_classes=1\n",
    "model = LSTM(num_classes, input_size, hidden_size, num_layers, X_train_tensors_f.shape[1])\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([13, 22])) that is different to the input size (torch.Size([84, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (84) must match the size of tensor b (13) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [153], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(X_train_tensors_f)\n\u001b[0;32m      3\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m----> 4\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, y_train_tensors)\n\u001b[0;32m      5\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m      7\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 536\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:3291\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3288\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3289\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3291\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[0;32m   3292\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\torch\\functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[1;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (84) must match the size of tensor b (13) at non-singleton dimension 0"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs) :\n",
    "    outputs = model.forward(X_train_tensors_f)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, y_train_tensors)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0 :\n",
    "        print(f'Epoch : {epoch}, loss : {loss.item():1.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_ss = ss.transform(data.iloc[:, :-1])\n",
    "df_y_ms = ms.transform(data.iloc[:, -1:])\n",
    "\n",
    "df_x_ss = Variable(torch.Tensor(df_x_ss))\n",
    "df_y_ms = Variable(torch.Tensor(df_y_ms))\n",
    "df_x_ss = torch.reshape(df_x_ss, (df_x_ss.shape[0], 1, df_x_ss.shape[1]))\n",
    "train_predict = model(df_x_ss)\n",
    "predicted = train_predict.data.numpy()\n",
    "label_y = df_y_ms.data.numpy()\n",
    "\n",
    "predicted = ms.inverse_transform(predicted)\n",
    "label_y = ms.inverse_transform(label_y)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.axvline(x=200, c='r', linestyle='--')\n",
    "\n",
    "plt.plot(label_y, label='Actual Data')\n",
    "plt.plot(predicted, label='Predicted Data')\n",
    "plt.title('Time-Series Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
